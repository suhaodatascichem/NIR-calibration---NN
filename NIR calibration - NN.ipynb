{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suhaodatascichem/NIR-calibration---NN/blob/main/NIR%20calibration%20-%20NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN2k8TijYmRz",
        "outputId": "87eafa75-f014-4c83-b5d1-a25180a19081",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting brukeropusreader\n",
            "  Downloading brukeropusreader-1.3.4.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from brukeropusreader) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from brukeropusreader) (1.13.1)\n",
            "Building wheels for collected packages: brukeropusreader\n",
            "  Building wheel for brukeropusreader (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for brukeropusreader: filename=brukeropusreader-1.3.4-py3-none-any.whl size=5303 sha256=215b373e8a98f62ea75ff9fee8f76c2ec2b1d91e721577b24f1b354976abf6c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/b6/88/6f85a1072c5d56dc11f6f28858fc7049281172fa48a5100908\n",
            "Successfully built brukeropusreader\n",
            "Installing collected packages: brukeropusreader\n",
            "Successfully installed brukeropusreader-1.3.4\n"
          ]
        }
      ],
      "source": [
        "# read bruker files - brukeropusreader\n",
        "!pip install brukeropusreader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from brukeropusreader import read_file\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "5U3bxt5T_BTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rFMLPkuP_iEq",
        "outputId": "3e8ad7ce-9a08-42a1-ff79-5ed3f2f7596f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mRllzaRiB-dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Read and Combine OPUS Files\n",
        "data_directory = \"C:/NIR calibration - NN\"\n",
        "\n",
        "def load_all_spectra(data_directory):\n",
        "    all_data = []\n",
        "\n",
        "    for file_name in os.listdir(data_directory):\n",
        "        if file_name.endswith(\".opus\"):  # Check for OPUS files\n",
        "            file_path = os.path.join(data_directory, file_name)\n",
        "            data = read_file(file_path)  # Read OPUS file\n",
        "            df = pd.DataFrame(data)  # Convert to DataFrame\n",
        "            all_data.append(df)\n",
        "\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)  # Merge all DataFrames\n",
        "    return combined_df\n",
        "\n",
        "# Load all OPUS files\n",
        "df = load_all_spectra(data_directory)\n",
        "\n",
        "# Step 2: Preprocess Data\n",
        "target_column = \"AB\"  # Ensure this column exists in your data\n",
        "X = df.drop(columns=[target_column])  # Independent variables\n",
        "y = df[target_column]  # Target variable\n",
        "\n",
        "# Normalize input features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Build Neural Network Model\n",
        "def build_model(input_shape):\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(64, activation=\"relu\", input_shape=(input_shape,)),\n",
        "        layers.Dense(32, activation=\"relu\"),\n",
        "        layers.Dense(1)  # Output layer (regression)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
        "    return model\n",
        "\n",
        "# Initialize and train the model\n",
        "model = build_model(X_train.shape[1])\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Step 4: Evaluate the model\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f\"Test MAE: {test_mae}\")\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "print(\"Predictions:\", predictions[:5])  # Show first 5 predictions\n"
      ],
      "metadata": {
        "id": "Gs2Kw52A--76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Read and Combine OPUS Files\n",
        "data_directory = \"C:/NIR calibration - NN\""
      ],
      "metadata": {
        "id": "KZFZgqdDC6o5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}